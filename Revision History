3.8 zdk :
我看了一下好像有一些问题。首先是可视化一开始出错了只会显示最后一个时间步的状态，我给他修好了还加
了一个拖拽栏方便观察。然后原来的逻辑里Level(灾情登记）和need rescue（需要救援的总时间）这两个
变量和逻辑和判断成功的逻辑不太对。我修改为仅有need rescue先到0才算成功。

发现原来在救援执行时，同时减少了两个值：
disasters[(target_x, target_y)]["level"] -= rescuer["capacity"] - 减少灾情严重程度
disasters[(target_x, target_y)]["rescue_needed"] -= 1 - 减少所需救援次数
现在修改为在救援执行时只减少rescue_needed。当 level=0 时，表示灾情自然结束了
但如果此时 rescue_needed 不为 0，说明灾情没有得到成功救助

3.10sjy：
改了一下救援人员的救援能力，使得救援能力强的先去重灾区

3.10zdk:
救援执行逻辑中，无论救援人员的capacity是多少，每次救援都是将灾情点的rescue_needed减1。
也就是说，当前系统中救援人员的capacity参数实际上没有被使用。

目前，level 和 rescue_needed 是独立随机生成的，没有关联性。修改为level越大，rescue_needed越大

修改了救援执行逻辑，使得救援人员的capacity参数被正确使用。救援能力强的先去重灾区。
计算能力匹配度：灾情点的rescue_needed与救援员capacity的比例，越接近1越好
                    capacity_match = min(
                        rescuer.get("capacity", 1) / task["data"].get("rescue_needed", 1),
                        1.0
                    )
                    
                    # 新的评分公式：结合灾情等级、能力匹配度和距离
                    score = (task["data"]["level"] * capacity_match) / (distance + 1)

首先救援成功率改为统计过去30个时间步中的成功率而不是全局成功率。
然后救援成功率的可视化与过程可视化一起显示且和拖拽条功能适配

在环境和灾情点中添加时间步信息。
修改environment.py中灾情点创建和初始化的部分，
确保每个灾情点记录其创建和结束时间步
保持向后兼容性，当没有提供时间步时仍可使用所有灾情

统计窗口大小的参数添加到配置文件中，方便地进行调整。

修改rescue_dispatch.py文件，在评分公式中加入对救援者移动速度的考虑。
改轻，中度灾情的排序逻辑，让它考虑到达时间而不仅仅是距离。

继续改进正确率统计的逻辑，现在正确率统计只考虑已完成灾情点：统计时只考虑已经完成（成功或失败）的灾情点，完全忽略进行中的灾情点。
窗口大小N现在明确指的是"最近N个已完成的灾情点"，而不是"最近N个时间步中的灾情点"
使得救援成功率的计算更加准确和有意义，特别是在灾情点稀疏或大部分灾情点仍在进行中的情况下。

在config.py中添加了三种预设的灾难规模：小型灾难，中型灾难，大型灾难
还可以选择自定义灾难，使用config.py中的自定义设置。
实现了灾情生成概率随时间衰减的功能
添加了相关的配置参数，可以控制衰减过程

3.11 zdk:
根据前面已经基本完成的模拟环境，初步构建了多智能体强化学习（MARL）的训练框架，现在已经没有明显bug，训练逻辑未作优化。
目前的训练逻辑
训练环境设置
训练分为三个阶段，使用adjust_disaster_settings函数动态调整灾难生成参数：
初期：灾难生成概率约0.5，维持至少20个灾难点
中期：灾难生成概率约0.3，维持5-20个灾难点
后期：灾难生成概率约0.1，维持不超过5个灾难点
每个训练轮次都会创建新的环境实例，确保独立性

使用RescuerAgent类实现每个救援人员的神经网络模型
网络结构：
特征提取层：两层全连接网络，包含ReLU激活函数
价值网络头：评估状态价值
策略网络头：产生动作概率
所有智能体由MARLController统一管理，实现协作训练

使用自定义训练循环custom_train_loop
在每个回合中：
更新环境状态，包括灾难点的生成和演变
对每个救援人员智能体：
获取当前状态
选择动作（使用ε-贪心策略）
执行动作并获取奖励
将经验存储到回放缓冲区
更新神经网络参数
记录训练统计数据

奖励机制
基础时间惩罚：每步-0.1，鼓励快速行动
完成救援奖励：+10.0，成功解决灾情
高优先级任务奖励：+level/10，优先处理高风险灾情
协调奖励：+2.0，当其他救援人员未前往同一灾情点
救援进度奖励：与灾情等级降低成正比

经验回放与网络更新
使用经验回放缓冲区存储转换样本(state, action, reward, next_state, done)
梯度下降优化，使用smooth_l1_loss计算损失
采用梯度裁剪防止梯度爆炸（效果存疑，可能影响训练效果）

MARL模型的部署主要通过marl_integration.py文件实现
提供多种集成策略：
纯MARL模式：所有救援人员使用强化学习
混合模式：部分救援人员使用强化学习，部分使用传统算法
传统算法模式：不使用强化学习

通过config.py设置部署选项：
TASK_ALLOCATION_ALGORITHM参数控制使用哪种分配策略
MARL_CONFIG包含模型路径和其他参数

重构了项目结构，将裸露的代码和配置文件分离，方便管理，将train和main作为接口外置。


3.13 zdk:

精确的灾难点管理机制
初期：灾难生成概率约0.5，维持至少20-50个灾难点(少于20个自动随机补充灾难点，多于50个自动随机减少灾难点）
中期：灾难生成概率约0.3，维持5-20个灾难点（少于5个自动补充，多于20个自动减少）
后期：灾难生成概率约0.1，维持不超过5不少于1个灾难点（超过5个自动减少，少于1个自动增加）