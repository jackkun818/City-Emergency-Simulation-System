import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
from collections import deque
import os
import io
import contextlib
import copy
import pickle
from src.core import config
import json
from src.core.environment import Environment
from gymnasium import spaces
from src.rl.rl_util import adjust_disaster_settings, calculate_reward
from src.utils.stats import calculate_rescue_success_rate

# å®šä¹‰ä¿å­˜ç›®å½•
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
SAVE_DIR = os.path.join(project_root, 'train_visualization_save')
os.makedirs(SAVE_DIR, exist_ok=True)

class Colors:
    HEADER = '\033[95m'     # ç²‰è‰²
    BLUE = '\033[94m'       # è“è‰²
    CYAN = '\033[96m'       # é’è‰²
    GREEN = '\033[92m'      # ç»¿è‰²
    YELLOW = '\033[93m'     # é»„è‰²
    RED = '\033[91m'        # çº¢è‰²
    ENDC = '\033[0m'        # ç»“æŸé¢œè‰²
    BOLD = '\033[1m'        # ç²—ä½“
    UNDERLINE = '\033[4m'   # ä¸‹åˆ’çº¿

    
class RescuerAgent(nn.Module):
    """å•ä¸ªæ•‘æ´äººå‘˜çš„æ™ºèƒ½ä½“æ¨¡å‹"""
    def __init__(self, state_dim, action_dim, hidden_dim=128, device=None):
        super(RescuerAgent, self).__init__()
        # è®¾ç½®è®¾å¤‡
        self.device = device if device is not None else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.to(self.device)
        
        self.feature_layer = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # ä»·å€¼ç½‘ç»œå¤´ - è¯„ä¼°çŠ¶æ€ä»·å€¼
        self.value_head = nn.Linear(hidden_dim, 1)
        
        # ç­–ç•¥ç½‘ç»œå¤´ - äº§ç”ŸåŠ¨ä½œæ¦‚ç‡
        self.policy_head = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, x):
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if not isinstance(x, torch.Tensor):
            x = torch.FloatTensor(x).to(self.device)
        elif x.device != self.device:
            x = x.to(self.device)
            
        features = self.feature_layer(x)
        value = self.value_head(features)
        policy_logits = self.policy_head(features)
        return value, policy_logits
    
    def get_action(self, state, epsilon=0.0):
        """ä½¿ç”¨Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if random.random() < epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return random.randint(0, self.policy_head.out_features - 1)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            with torch.no_grad():
                value, policy_logits = self.forward(state)
                return torch.argmax(policy_logits).item()


class ExperienceReplay:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, capacity):
        self.memory = deque(maxlen=capacity)
        
    def push(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
        
    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)


class MARLController:
    """å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨ï¼Œç®¡ç†æ‰€æœ‰æ•‘æ´äººå‘˜æ™ºèƒ½ä½“"""
    def __init__(self, env_or_grid_size=None, num_rescuers=None, hidden_dim=128, lr=0.001, gamma=0.99):
        """åˆå§‹åŒ–MARLæ§åˆ¶å™¨
        
        Args:
            env_or_grid_size: å¯ä»¥æ˜¯Environmentå¯¹è±¡æˆ–grid_sizeæ•°å€¼
            num_rescuers: æ•‘æ´äººå‘˜æ•°é‡æˆ–æ•‘æ´äººå‘˜åˆ—è¡¨
            hidden_dim: éšè—å±‚ç»´åº¦
            lr: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
        """
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å¦æ˜¯Environmentå¯¹è±¡
        if hasattr(env_or_grid_size, 'GRID_SIZE') and hasattr(env_or_grid_size, 'rescuers'):
            # å¦‚æœæ˜¯Environmentå¯¹è±¡ï¼Œä»ä¸­æå–å‚æ•°
            self.grid_size = env_or_grid_size.GRID_SIZE
            self.num_rescuers = len(env_or_grid_size.rescuers)
        else:
            # å¦åˆ™æŒ‰åŸæ¥çš„é€»è¾‘å¤„ç†
            self.grid_size = env_or_grid_size or config.get_config_param("grid_size")
            self.num_rescuers = num_rescuers or config.get_config_param("num_rescuers")
            if isinstance(num_rescuers, list):  # å¦‚æœä¼ å…¥çš„æ˜¯rescuersåˆ—è¡¨
                self.num_rescuers = len(num_rescuers)
        
        self.gamma = gamma
        
        # è®¾ç½®è®¾å¤‡
        self._device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # çŠ¶æ€ç©ºé—´ï¼š
        # 1. æ•‘æ´äººå‘˜ä½ç½® (x, y)
        # 2. æ•‘æ´äººå‘˜ä¸“é•¿ç±»å‹
        # 3. æ•‘æ´äººå‘˜æ•ˆèƒ½å€¼
        # 4. æ¯ä¸ªç¾æƒ…ç‚¹çš„ç±»å‹å’Œä¸¥é‡ç¨‹åº¦
        # 5. æ¯ä¸ªç¾æƒ…ç‚¹çš„å½“å‰æ•‘æ´èµ„æºåˆ†é…æƒ…å†µ
        # çŠ¶æ€ç»´åº¦è®¡ç®—: æ•‘æ´äººå‘˜è‡ªèº«çŠ¶æ€(4) + ç¾æƒ…ä¿¡æ¯(grid_size*grid_size*5)
        self.state_dim = 4 + self.grid_size * self.grid_size * 5
        
        # åŠ¨ä½œç©ºé—´ï¼š
        # 0: ä¸åŠ¨
        # 1~grid_size*grid_size: å‰å¾€å¯¹åº”æ ¼å­åæ ‡çš„ç¾æƒ…ç‚¹
        self.action_dim = 1 + self.grid_size * self.grid_size
        
        # ä¸ºæ¯ä¸ªæ•‘æ´äººå‘˜åˆ›å»ºä¸€ä¸ªæ™ºèƒ½ä½“
        self.agents = [RescuerAgent(self.state_dim, self.action_dim, hidden_dim, device=self._device) for _ in range(self.num_rescuers)]
        self.optimizers = [optim.Adam(agent.parameters(), lr=lr) for agent in self.agents]
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffers = [ExperienceReplay(config.MARL_CONFIG["replay_buffer_size"]) for _ in range(self.num_rescuers)]
        
        # ç›®æ ‡ç½‘ç»œ
        self.target_agents = [RescuerAgent(self.state_dim, self.action_dim, hidden_dim, device=self._device) for _ in range(self.num_rescuers)]
        for i in range(self.num_rescuers):
            self.target_agents[i].load_state_dict(self.agents[i].state_dict())

        # æ›´æ–°è®¡æ•°å™¨ï¼Œç”¨äºå®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.update_counter = 0
        
        # è®­ç»ƒå‚æ•°
        self.epsilon = config.EPSILON_START if hasattr(config, "EPSILON_START") else config.MARL_CONFIG["epsilon_start"]
        self.epsilon_start = config.EPSILON_START if hasattr(config, "EPSILON_START") else config.MARL_CONFIG["epsilon_start"]
        self.epsilon_end = config.EPSILON_FINAL if hasattr(config, "EPSILON_FINAL") else config.MARL_CONFIG["epsilon_end"]
        self.epsilon_decay = config.MARL_CONFIG["epsilon_decay"]
        self.batch_size = config.MARL_CONFIG["batch_size"]
        self.target_update = config.MARL_CONFIG["target_update"]
        self.steps_done = 0
        
        # ç¯å¢ƒç¼“å­˜ï¼Œç”¨äºåè°ƒ
        self._env_cache = []
        
        # å¥–åŠ±è·Ÿè¸ª
        self.reward_tracking = {
            "completion_reward": [],  # å®Œæˆä»»åŠ¡å¥–åŠ±
            "priority_reward": [],    # é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¥–åŠ±
            "coordination_reward": [], # åè°ƒå¥–åŠ±
            "progress_reward": [],    # æ•‘æ´è¿›åº¦å¥–åŠ±
            "time_penalty": []        # æ—¶é—´æƒ©ç½š
        }
        
        # ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
        os.makedirs(os.path.dirname(config.MARL_CONFIG["model_save_path"]), exist_ok=True)
    
    @property
    def device(self):
        """è·å–å½“å‰è®¾å¤‡"""
        return self._device
    
    def build_state(self, rescuer_idx, rescuers, disasters):
        """æ„å»ºå•ä¸ªæ™ºèƒ½ä½“çš„çŠ¶æ€è¡¨ç¤º"""
        if rescuer_idx >= len(rescuers):
            print(f"è­¦å‘Š: rescuer_idx ({rescuer_idx}) è¶…å‡ºäº† rescuers åˆ—è¡¨é•¿åº¦ ({len(rescuers)})")
            # è¿”å›é›¶å‘é‡ä½œä¸ºåº”æ€¥æªæ–½
            return torch.zeros((1, self.state_dim), device=self.device)
            
        rescuer = rescuers[rescuer_idx]
        
        # æå–æ•‘æ´äººå‘˜è‡ªèº«çŠ¶æ€
        state = [
            rescuer["position"][0] / self.grid_size,  # æ ‡å‡†åŒ–xåæ ‡
            rescuer["position"][1] / self.grid_size,  # æ ‡å‡†åŒ–yåæ ‡
            1.0 if "target" in rescuer and rescuer["target"] is not None else 0.0,  # æ˜¯å¦å·²åˆ†é…ä»»åŠ¡
            rescuer.get("speed", config.MAX_SPEED) / config.MAX_SPEED  # æ ‡å‡†åŒ–ç§»åŠ¨é€Ÿåº¦
        ]
        
        # ç½‘æ ¼çŠ¶æ€çŸ©é˜µåˆå§‹åŒ–
        grid_state = np.zeros((self.grid_size, self.grid_size, 5))
        
        # å¡«å……ç¾æƒ…ä¿¡æ¯
        for pos, disaster in disasters.items():
            x, y = pos
            if 0 <= x < self.grid_size and 0 <= y < self.grid_size:
                # ç¾æƒ…å­˜åœ¨æ ‡å¿—
                grid_state[x, y, 0] = 1.0  
                # æ ‡å‡†åŒ–ç¾æƒ…ç­‰çº§
                grid_state[x, y, 1] = disaster["level"] / config.CRITICAL_DISASTER_THRESHOLD  
                # æ ‡å‡†åŒ–æ‰€éœ€æ•‘æ´èµ„æº
                grid_state[x, y, 2] = disaster["rescue_needed"] / config.MAX_RESCUE_CAPACITY  
                # æ ‡å‡†åŒ–å·²åˆ†é…èµ„æº
                assigned_count = sum(1 for r in rescuers if "target" in r and r["target"] == pos)
                grid_state[x, y, 3] = assigned_count / self.num_rescuers  
                # å‰©ä½™æ‰€éœ€æ•‘æ´èµ„æº
                grid_state[x, y, 4] = disaster["rescue_needed"] / config.MAX_RESCUE_CAPACITY
        
        # å°†ç½‘æ ¼çŠ¶æ€å±•å¹³å¹¶æ·»åŠ åˆ°çŠ¶æ€å‘é‡
        grid_state_flat = grid_state.flatten()
        state.extend(grid_state_flat)
        
        return torch.FloatTensor(state).unsqueeze(0).to(self.device)  # æ·»åŠ æ‰¹å¤„ç†ç»´åº¦å¹¶ç§»è‡³æ­£ç¡®è®¾å¤‡
    
    def select_action(self, state, rescuer_idx, disasters=None):
        """ä¸ºå•ä¸ªæ•‘æ´äººå‘˜é€‰æ‹©åŠ¨ä½œï¼Œä½¿ç”¨åŠ¨æ€åŠ¨ä½œæ©ç """
        # æ£€æŸ¥rescuer_idxæ˜¯å¦è¶…å‡ºèŒƒå›´
        if rescuer_idx >= self.num_rescuers:
            print(f"è­¦å‘Š: rescuer_idx ({rescuer_idx}) è¶…å‡ºäº†èŒƒå›´ ({self.num_rescuers})")
            return 0  # è¿”å›ä¸åŠ¨ä½œ
            
        # ç¡®ä¿stateæ˜¯å¼ é‡ä¸”åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if not isinstance(state, torch.Tensor):
            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        elif state.device != self.device:
            state = state.to(self.device)
        
        # åˆ›å»ºåŠ¨ä½œæ©ç  - åˆå§‹åŒ–æ‰€æœ‰åŠ¨ä½œä¸ºæ— æ•ˆ
        action_mask = torch.zeros(self.action_dim, device=self.device)
        
        # å¦‚æœæä¾›äº†ç¾æƒ…ä¿¡æ¯ï¼Œåˆ›å»ºåŠ¨æ€æ©ç 
        if disasters is not None and len(disasters) > 0:
            # å¯¹æ¯ä¸ªæ´»è·ƒçš„ç¾æƒ…ç‚¹ï¼Œå°†å¯¹åº”çš„åŠ¨ä½œæ ‡è®°ä¸ºæœ‰æ•ˆ
            for pos, disaster in disasters.items():
                # åªæœ‰å½“ç¾éš¾ç‚¹ä»éœ€è¦æ•‘æ´æ—¶æ‰æ ‡è®°ä¸ºæœ‰æ•ˆåŠ¨ä½œ
                if disaster.get("rescue_needed", 0) > 0:
                    x, y = pos
                    if 0 <= x < self.grid_size and 0 <= y < self.grid_size:
                        action_idx = x * self.grid_size + y + 1  # +1 æ˜¯å› ä¸ºåŠ¨ä½œ0æ˜¯"ä¸åŠ¨"
                        if 0 < action_idx < self.action_dim:  # ç¡®ä¿åŠ¨ä½œç´¢å¼•æœ‰æ•ˆ
                            action_mask[action_idx] = 1.0
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œï¼ˆæ²¡æœ‰ç¾æƒ…ç‚¹ï¼‰ï¼Œåˆ™ä¸åŠ¨
            if action_mask.sum() == 0:
                return 0
        else:
            # å¦‚æœæ²¡æœ‰æä¾›ç¾æƒ…ä¿¡æ¯ï¼Œå…è®¸æ‰€æœ‰åŠ¨ä½œï¼ˆé™¤äº†ä¸åŠ¨ï¼‰
            action_mask[1:] = 1.0
            
        # ä½¿ç”¨epsilon-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        if random.random() < self.epsilon:
            # æ¢ç´¢ï¼šä»æœ‰æ•ˆåŠ¨ä½œä¸­éšæœºé€‰æ‹©
            valid_actions = torch.nonzero(action_mask).squeeze(-1)
            if len(valid_actions) > 0:
                # ä»æœ‰æ•ˆåŠ¨ä½œä¸­éšæœºé€‰æ‹©
                random_idx = random.randint(0, len(valid_actions) - 1)
                return valid_actions[random_idx].item()
            else:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåŠ¨ä½œï¼Œåˆ™ä¸åŠ¨
                return 0
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„æœ‰æ•ˆåŠ¨ä½œ
            with torch.no_grad():
                _, policy_logits = self.agents[rescuer_idx](state)
                
                # åº”ç”¨æ©ç ï¼šå°†æ— æ•ˆåŠ¨ä½œçš„logitsè®¾ä¸ºè´Ÿæ— ç©·
                masked_logits = policy_logits.clone()
                masked_logits[0, action_mask == 0] = float('-inf')
                
                # å¦‚æœæ‰€æœ‰åŠ¨ä½œéƒ½è¢«æ©ç ï¼Œåˆ™è¿”å›ä¸åŠ¨ä½œ
                if (masked_logits[0] == float('-inf')).all():
                    return 0
                
                # é€‰æ‹©æœ€å¤§Qå€¼çš„æœ‰æ•ˆåŠ¨ä½œ
                return torch.argmax(masked_logits).item()
    
    def select_actions(self, rescuers, disasters, training=False):
        """ä¸ºæ‰€æœ‰æ•‘æ´äººå‘˜é€‰æ‹©åŠ¨ä½œï¼Œå…¼å®¹æ—§ç‰ˆæœ¬API"""
        actions = []
        
        # æ›´æ–°æ¢ç´¢ç‡
        if training:
            self.steps_done += 1
        
        for i in range(self.num_rescuers):
            # æ£€æŸ¥æ•‘æ´äººå‘˜æ˜¯å¦æ­£åœ¨æ‰§è¡Œæ•‘æ´ä»»åŠ¡
            if i < len(rescuers) and rescuers[i].get("actively_rescuing", False):
                # å¦‚æœæ­£åœ¨æ‰§è¡Œæ•‘æ´ï¼Œè¿”å›ä¸åŠ¨ä½œï¼ˆä¿æŒå½“å‰çŠ¶æ€ï¼‰
                actions.append(0)
                continue
                
            state = self.build_state(i, rescuers, disasters)
            # ä½¿ç”¨select_actionæ–¹æ³•é€‰æ‹©åŠ¨ä½œï¼Œä¼ é€’ç¾æƒ…ä¿¡æ¯
            action = self.select_action(state, i, disasters)
            actions.append(action)
            
            # å°†åŠ¨ä½œè½¬æ¢ä¸ºç›®æ ‡ä½ç½®å¹¶åˆ†é…ç»™æ•‘æ´äººå‘˜
            if action > 0 and i < len(rescuers):  # é0åŠ¨ä½œè¡¨ç¤ºå‰å¾€æŸä¸ªä½ç½®
                action_idx = action - 1
                x = action_idx // self.grid_size
                y = action_idx % self.grid_size
                target_pos = (x, y)
                
                # å¦‚æœç›®æ ‡ä½ç½®æœ‰æ´»è·ƒçš„ç¾æƒ…ï¼Œåˆ™åˆ†é…ä»»åŠ¡
                if target_pos in disasters and disasters[target_pos].get("rescue_needed", 0) > 0:
                    rescuers[i]["target"] = target_pos
                else:
                    # å¦‚æœç›®æ ‡ä½ç½®æ²¡æœ‰æ´»è·ƒç¾æƒ…ï¼Œåˆ™æ¸…é™¤å½“å‰ä»»åŠ¡
                    if "target" in rescuers[i]:
                        del rescuers[i]["target"]
            elif action == 0 and i < len(rescuers):
                # å¦‚æœé€‰æ‹©ä¸åŠ¨ä¸”æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„æ•‘æ´ï¼Œæ¸…é™¤ç›®æ ‡
                if not rescuers[i].get("actively_rescuing", False) and "target" in rescuers[i]:
                    del rescuers[i]["target"]
        
        return actions
    
    def store_transition(self, state, action, reward, next_state, done, rescuer_idx):
        """å­˜å‚¨å•ä¸ªæ™ºèƒ½ä½“çš„ç»éªŒ"""
        # æ£€æŸ¥rescuer_idxæ˜¯å¦è¶…å‡ºèŒƒå›´
        if rescuer_idx >= self.num_rescuers:
            print(f"è­¦å‘Š: store_transition - rescuer_idx ({rescuer_idx}) è¶…å‡ºäº†èŒƒå›´ ({self.num_rescuers})")
            return
            
        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        if isinstance(state, torch.Tensor):
            state = state.cpu().numpy()
        if isinstance(next_state, torch.Tensor):
            next_state = next_state.cpu().numpy()
            
        self.replay_buffers[rescuer_idx].push(state, action, reward, next_state, done)
    
    def store_experience(self, rescuer_idx, state, action, reward, next_state, done):
        """å…¼å®¹æ—§ç‰ˆæœ¬APIçš„ç»éªŒå­˜å‚¨æ–¹æ³•"""
        self.store_transition(state, action, reward, next_state, done, rescuer_idx)
    
    def update_agent(self, agent_idx):
        """æ›´æ–°å•ä¸ªæ™ºèƒ½ä½“"""
        if agent_idx >= self.num_rescuers:
            print(f"è­¦å‘Š: update_agent - agent_idx ({agent_idx}) è¶…å‡ºäº†èŒƒå›´ ({self.num_rescuers})")
            return 0.0
            
        if len(self.replay_buffers[agent_idx]) < self.batch_size:
            return 0.0
        
        # ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·
        transitions = self.replay_buffers[agent_idx].sample(self.batch_size)
        batch = list(zip(*transitions))
        
        # å‡†å¤‡æ‰¹æ•°æ®
        state_batch = torch.FloatTensor(np.array(batch[0])).to(self.device)
        # ç¡®ä¿state_batchå½¢çŠ¶æ­£ç¡® - å¦‚æœstateæ˜¯äºŒç»´å¼ é‡(batch_size, 1, state_dim)ï¼Œåˆ™éœ€è¦å‹ç¼©ä¸€ç»´
        if len(state_batch.shape) == 3 and state_batch.shape[1] == 1:
            state_batch = state_batch.squeeze(1)
            
        # ç¡®ä¿action_batchæ˜¯é•¿æ•´å‹å¹¶ä¸”ç»´åº¦æ­£ç¡®
        action_batch = torch.LongTensor(batch[1]).to(self.device)
        # æ£€æŸ¥å¹¶ç¡®ä¿action_batchæ˜¯äºŒç»´çš„
        if len(action_batch.shape) == 1:
            action_batch = action_batch.unsqueeze(1)  # æ·»åŠ ç¬¬äºŒç»´
        
        reward_batch = torch.FloatTensor(batch[2]).unsqueeze(1).to(self.device)
        
        # æ£€æŸ¥å¥–åŠ±æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        if torch.isnan(reward_batch).any() or torch.isinf(reward_batch).any():
            # æ›¿æ¢NaNæˆ–æ— é™å€¼ä¸º0
            reward_batch = torch.nan_to_num(reward_batch, nan=0.0, posinf=10.0, neginf=-10.0)
        
        # åŒæ ·å¤„ç†next_state_batch
        next_state_batch = torch.FloatTensor(np.array(batch[3])).to(self.device)
        if len(next_state_batch.shape) == 3 and next_state_batch.shape[1] == 1:
            next_state_batch = next_state_batch.squeeze(1)
            
        done_batch = torch.FloatTensor(batch[4]).unsqueeze(1).to(self.device)
        
        try:
            # è®¡ç®—å½“å‰Qå€¼
            current_value, policy_logits = self.agents[agent_idx](state_batch)
            
            # ç¡®ä¿actionsåœ¨æœ‰æ•ˆèŒƒå›´å†…
            action_batch = torch.clamp(action_batch, 0, self.action_dim - 1)
            
            # ç°åœ¨gatheræ“ä½œåº”è¯¥èƒ½æ­£ç¡®æ‰§è¡Œ
            q_values = policy_logits.gather(1, action_batch)
            
            # è®¡ç®—ç›®æ ‡Qå€¼
            with torch.no_grad():
                next_value, next_policy_logits = self.target_agents[agent_idx](next_state_batch)
                next_q_values = next_policy_logits.max(1, keepdim=True)[0]
                expected_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)
            
            # æ£€æŸ¥Qå€¼æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if torch.isnan(q_values).any() or torch.isinf(q_values).any():
                # å¦‚æœå‘ç°å¼‚å¸¸å€¼ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤æŸå¤±å€¼ï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°
                print(f"è­¦å‘Šï¼šæ™ºèƒ½ä½“ {agent_idx} çš„Qå€¼åŒ…å«NaNæˆ–æ— é™å€¼ï¼Œè·³è¿‡æ›´æ–°")
                return 0.0
                
            # æ£€æŸ¥ç›®æ ‡Qå€¼æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if torch.isnan(expected_q_values).any() or torch.isinf(expected_q_values).any():
                # å¦‚æœå‘ç°å¼‚å¸¸å€¼ï¼Œå¤„ç†ç›®æ ‡Qå€¼
                expected_q_values = torch.nan_to_num(expected_q_values, nan=0.0, posinf=10.0, neginf=-10.0)
            
            # è®¡ç®—æŸå¤± - ä½¿ç”¨åˆé€‚çš„æŸå¤±å‡½æ•°
            loss = F.smooth_l1_loss(q_values, expected_q_values)
            
            # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦å¼‚å¸¸
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"è­¦å‘Šï¼šæ™ºèƒ½ä½“ {agent_idx} çš„æŸå¤±å€¼ä¸º {loss.item()}ï¼Œè·³è¿‡æ›´æ–°")
                return 0.0
                
            # é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œè®¾ç½®æŸå¤±ä¸Šé™
            loss_value = loss.item()
            if loss_value > 1000.0:
                # ç¼©æ”¾æŸå¤±ä»¥é¿å…æå¤§å€¼
                loss = loss * (1000.0 / loss_value)
                
            # ä¼˜åŒ–æ¨¡å‹
            self.optimizers[agent_idx].zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(self.agents[agent_idx].parameters(), max_norm=10.0)
                
            self.optimizers[agent_idx].step()
            
            return min(loss.item(), 1000.0)  # è¿”å›æœ‰é™çš„æŸå¤±å€¼
            
        except Exception as e:
            print(f"æ›´æ–°æ™ºèƒ½ä½“ {agent_idx} æ—¶å‡ºé”™: {e}")
            return 0.0
    
    def update_agents(self):
        """æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“"""
        total_loss = 0.0
        updated_count = 0
        
        for i in range(self.num_rescuers):
            loss = self.update_agent(i)
            if loss > 0:
                total_loss += loss
                updated_count += 1
            
            # å¦‚æœæ»¡è¶³æ›´æ–°ç›®æ ‡ç½‘ç»œçš„æ¡ä»¶ï¼Œåˆ™æ›´æ–°ç›®æ ‡ç½‘ç»œ
            if self.steps_done % self.target_update == 0:
                self.target_agents[i].load_state_dict(self.agents[i].state_dict())
        
        # è¿”å›å¹³å‡æŸå¤±ï¼Œç”¨äºç›‘æ§è®­ç»ƒè¿›åº¦
        return total_loss / max(1, updated_count) if updated_count > 0 else 0.0
    
    def track_rewards(self, reward_info):
        """è·Ÿè¸ªä¸åŒç±»å‹çš„å¥–åŠ±"""
        for reward_type, value in reward_info.items():
            if reward_type in self.reward_tracking:
                self.reward_tracking[reward_type].append(value)
    
    def get_reward_stats(self, reset=True):
        """è·å–å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯å¹¶å¯é€‰æ‹©æ€§åœ°é‡ç½®è·Ÿè¸ªæ•°æ®"""
        stats = {}
        for reward_type, values in self.reward_tracking.items():
            if values:  # å¦‚æœæœ‰æ•°æ®
                stats[reward_type] = {
                    "total": sum(values),
                    "avg": sum(values) / len(values),
                    "min": min(values),
                    "max": max(values)
                }
            else:
                stats[reward_type] = {"total": 0, "avg": 0, "min": 0, "max": 0}
                
        if reset:
            # é‡ç½®è·Ÿè¸ªæ•°æ®
            for reward_type in self.reward_tracking:
                self.reward_tracking[reward_type] = []
                
        return stats
    
    def save_models(self, path=None):
        """ä¿å­˜æ‰€æœ‰æ™ºèƒ½ä½“æ¨¡å‹"""
        if path is None:
            path = config.MARL_CONFIG["model_save_path"]
        
        model_states = {
            f"agent_{i}": self.agents[i].state_dict() for i in range(self.num_rescuers)
        }
        torch.save(model_states, path)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {path}")
    
    def load_models(self, path=None):
        """åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“æ¨¡å‹"""
        if path is None:
            path = config.MARL_CONFIG["model_save_path"]
        
        if not os.path.exists(path):
            print(f"æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {path}")
            return False
        
        try:
            model_states = torch.load(path)
            for i in range(self.num_rescuers):
                if f"agent_{i}" in model_states:
                    self.agents[i].load_state_dict(model_states[f"agent_{i}"])
                    self.target_agents[i].load_state_dict(model_states[f"agent_{i}"])
            print(f"æ¨¡å‹å·²ä» {path} åŠ è½½")
            return True
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False

    def action_to_target(self, action, grid_size=None):
        """
        å°†åŠ¨ä½œç´¢å¼•è½¬æ¢ä¸ºç›®æ ‡ä½ç½®åæ ‡
        
        å‚æ•°:
            action: åŠ¨ä½œç´¢å¼•ï¼ˆ0è¡¨ç¤ºä¸åŠ¨ï¼Œ1~grid_size*grid_sizeè¡¨ç¤ºç›®æ ‡ä½ç½®ï¼‰
            grid_size: ç½‘æ ¼å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.grid_size
            
        è¿”å›:
            (x, y): ç›®æ ‡ä½ç½®åæ ‡ï¼Œå¦‚æœactionä¸º0åˆ™è¿”å›None
        """
        if action <= 0:
            return None
            
        if grid_size is None:
            grid_size = self.grid_size
            
        action_idx = action - 1
        x = action_idx // grid_size
        y = action_idx % grid_size
        
        return (x, y)


class RescueEnvironment:
    def __init__(self, grid_size=None, num_rescuers=None, rescuers_data=None):
        """åˆå§‹åŒ–æ•‘æ´ç¯å¢ƒ"""
        self.env = Environment(grid_size=grid_size, num_rescuers=num_rescuers, rescuers_data=rescuers_data)
        # ä¿®å¤åŠ¨ä½œç©ºé—´å®šä¹‰ï¼Œä½¿å…¶ä¸MARLControllerä¿æŒä¸€è‡´
        grid_size = grid_size or config.get_config_param("grid_size")
        self.action_space = spaces.Discrete(1 + grid_size * grid_size)  # 0=ä¸åŠ¨ï¼Œ1~grid_size*grid_size=å‰å¾€å¯¹åº”æ ¼å­åæ ‡
        # è®¾ç½®è§‚å¯Ÿç©ºé—´ (å¤šä¸ªç‰¹å¾å¹³é¢: ç¾æƒ…, æ•‘æ´è€…ä½ç½®ç­‰)
        self.observation_space = spaces.Box(low=0, high=1, shape=(grid_size, grid_size, 3), dtype=np.float32)
        self.current_time_step = 0
        self.previous_success_count = 0
        self.disaster_count = 0

    def get_rescuers_data(self):
        """è·å–æ•‘æ´äººå‘˜æ•°æ®ï¼Œç”¨äºä¿å­˜"""
        return self.env.rescuers
    
    def save_rescuers_data(self, file_path):
        """ä¿å­˜æ•‘æ´äººå‘˜æ•°æ®åˆ°æ–‡ä»¶"""
        rescuers_data = self.get_rescuers_data()
        # å°†rescuersè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸åˆ—è¡¨
        serialized_rescuers = []
        for rescuer in rescuers_data:
            # æ£€æŸ¥rescueræ˜¯å¦ä¸ºå­—å…¸ç±»å‹
            if isinstance(rescuer, dict):
                # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥å¤åˆ¶
                rescuer_dict = rescuer.copy()
                # ç¡®ä¿positionæ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆJSONä¸æ”¯æŒå…ƒç»„ï¼‰
                if 'position' in rescuer_dict and isinstance(rescuer_dict['position'], tuple):
                    rescuer_dict['position'] = list(rescuer_dict['position'])
            else:
                # å¦‚æœæ˜¯å¯¹è±¡ï¼Œæå–å±æ€§
                rescuer_dict = {
                    'id': rescuer.id,
                    'position': list(rescuer.position) if hasattr(rescuer, 'position') else [0, 0],
                    'specialty': rescuer.specialty if hasattr(rescuer, 'specialty') else None,
                    'effectiveness': rescuer.effectiveness if hasattr(rescuer, 'effectiveness') else 1.0,
                    'capacity': rescuer.capacity if hasattr(rescuer, 'capacity') else 1,
                    'speed': rescuer.speed if hasattr(rescuer, 'speed') else 1,
                    'active_time': rescuer.active_time if hasattr(rescuer, 'active_time') else 0
                }
            serialized_rescuers.append(rescuer_dict)
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open(file_path, 'w') as f:
            json.dump(serialized_rescuers, f, indent=4)
        
        return serialized_rescuers
        
    @staticmethod
    def load_rescuers_data(file_path):
        """ä»æ–‡ä»¶åŠ è½½æ•‘æ´äººå‘˜æ•°æ®"""
        with open(file_path, 'r') as f:
            rescuers_data = json.load(f)
        
        # å°†æ•°æ®è½¬æ¢ä¸ºé€‚å½“çš„æ ¼å¼
        rescuers = []
        for rescuer_dict in rescuers_data:
            # ç¡®ä¿positionæ˜¯å…ƒç»„æ ¼å¼
            if 'position' in rescuer_dict and isinstance(rescuer_dict['position'], list):
                rescuer_dict['position'] = tuple(rescuer_dict['position'])
            rescuers.append(rescuer_dict)
        
        return rescuers

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        # è¿™ä¸ªæ–¹æ³•åœ¨ç°æœ‰çš„ç³»ç»Ÿä¸­ä¸éœ€è¦å®ç°ï¼Œå› ä¸ºç¯å¢ƒå·²ç»åœ¨main.pyä¸­åˆå§‹åŒ–
        self.current_time_step = 0
        return self.env
    
    def get_state_for_rescuer(self, rescuer_idx):
        """è·å–æŒ‡å®šæ•‘æ´äººå‘˜çš„çŠ¶æ€"""
        if not hasattr(self, 'marl_controller') or self.marl_controller is None:
            # å¦‚æœæ²¡æœ‰MARLæ§åˆ¶å™¨å®ä¾‹ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„
            from src.rl.marl_rescue import MARLController
            self.marl_controller = MARLController(
                env_or_grid_size=self.env.GRID_SIZE,
                num_rescuers=len(self.env.rescuers)
            )
        
        # ä½¿ç”¨MARLæ§åˆ¶å™¨çš„çŠ¶æ€æ„å»ºæ–¹æ³•
        return self.marl_controller.build_state(rescuer_idx, self.env.rescuers, self.env.disasters)
    
    def is_episode_done(self):
        """æ£€æŸ¥å½“å‰å›åˆæ˜¯å¦ç»“æŸ"""
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç¾éš¾éƒ½å·²è§£å†³
        all_resolved = True
        for disaster in self.env.disasters.values():
            if disaster["rescue_needed"] > 0:
                all_resolved = False
                break
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ—¶é—´æ­¥
        time_limit_reached = self.current_time_step >= config.SIMULATION_TIME - 1
        
        return all_resolved or time_limit_reached
    
    def step(self, rescuer_idx, action):
        """æ‰§è¡ŒæŒ‡å®šæ•‘æ´äººå‘˜çš„åŠ¨ä½œå¹¶è¿”å›ç»“æœ"""
        # è®°å½•æ—§çŠ¶æ€å’Œå½“å‰ç¾æƒ…ä¿¡æ¯ä»¥è®¡ç®—å¥–åŠ±
        old_state = self.env.rescuers[rescuer_idx].copy() if rescuer_idx < len(self.env.rescuers) else {}
        old_disasters = {pos: disaster.copy() for pos, disaster in self.env.disasters.items()}
        
        # æ£€æŸ¥æ•‘æ´äººå‘˜æ˜¯å¦æ­£åœ¨æ‰§è¡Œæ•‘æ´ä»»åŠ¡
        if rescuer_idx < len(self.env.rescuers) and self.env.rescuers[rescuer_idx].get("actively_rescuing", False):
            # å¦‚æœæ­£åœ¨æ‰§è¡Œæ•‘æ´ï¼Œå¿½ç•¥æ–°çš„åŠ¨ä½œåˆ†é…ï¼Œç»§ç»­å½“å‰æ•‘æ´
            pass
        else:
            # å°†åŠ¨ä½œè½¬æ¢ä¸ºç›®æ ‡ä½ç½®
            target_pos = None
            if action > 0:  # é0åŠ¨ä½œè¡¨ç¤ºå‰å¾€æŸä¸ªä½ç½®
                action_idx = action - 1
                x = action_idx // self.env.GRID_SIZE
                y = action_idx % self.env.GRID_SIZE
                target_pos = (x, y)
                
                # å¦‚æœç›®æ ‡ä½ç½®æœ‰æ´»è·ƒçš„ç¾æƒ…ï¼Œåˆ™åˆ†é…ä»»åŠ¡
                if target_pos in self.env.disasters and self.env.disasters[target_pos].get("rescue_needed", 0) > 0:
                    self.env.rescuers[rescuer_idx]["target"] = target_pos
                else:
                    # å¦‚æœç›®æ ‡ä½ç½®æ²¡æœ‰æ´»è·ƒç¾æƒ…ï¼Œåˆ™æ¸…é™¤å½“å‰ä»»åŠ¡
                    if "target" in self.env.rescuers[rescuer_idx]:
                        del self.env.rescuers[rescuer_idx]["target"]
        
        # æ‰§è¡Œæ•‘æ´
        from src.core.rescue_execution import execute_rescue
        execute_rescue(self.env.rescuers, self.env.disasters, self.env.GRID_SIZE, current_time_step=self.current_time_step)
        
        # è®¡ç®—å¥–åŠ±
        reward, reward_info = calculate_reward(self.env, rescuer_idx, old_state, old_disasters)
        
        # è·å–æ–°çŠ¶æ€
        next_state = self.get_state_for_rescuer(rescuer_idx)
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self.is_episode_done()
        
        # æ„å»ºä¿¡æ¯å­—å…¸
        info = {
            "success": False,
            "response_time": 0
        }
        
        # å¦‚æœæœ‰ç›®æ ‡ä¸”ç›®æ ‡æ˜¯ç¾æƒ…ç‚¹ä¸”ç¾æƒ…å·²è§£å†³ï¼Œåˆ™è¡¨ç¤ºæˆåŠŸ
        if "target" in self.env.rescuers[rescuer_idx]:
            target = self.env.rescuers[rescuer_idx]["target"]
            if target in old_disasters and target in self.env.disasters:
                old_disaster = old_disasters[target]
                current_disaster = self.env.disasters[target]
                
                # å¦‚æœç¾æƒ…è§£å†³æˆ–å‡è½»
                if old_disaster["rescue_needed"] > current_disaster["rescue_needed"]:
                    info["success"] = True
                    # è®¡ç®—å“åº”æ—¶é—´ - ä½¿ç”¨time_stepè€Œä¸æ˜¯start_time
                    if "time_step" in current_disaster:
                        response_time = self.current_time_step - current_disaster["time_step"]
                        # ç¡®ä¿å“åº”æ—¶é—´ä¸ºæ­£æ•°
                        if response_time <= 0:
                            print(f"è­¦å‘Šï¼šæ£€æµ‹åˆ°è´Ÿå“åº”æ—¶é—´ï¼Œcurrent_time_step={self.current_time_step}, disaster_time_step={current_disaster['time_step']}")
                            response_time = 1  # ç¡®ä¿è‡³å°‘ä¸º1
                        info["response_time"] = response_time
                    # å¦‚æœæ‰¾ä¸åˆ°time_stepï¼Œåˆ™ä½¿ç”¨ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼
                    else:
                        # ç¡®ä¿æœ‰ä¸€ä¸ªæœ‰æ•ˆçš„å“åº”æ—¶é—´
                        info["response_time"] = max(1, min(10, self.current_time_step))  # ä½¿ç”¨1åˆ°10ä¹‹é—´çš„åˆç†å€¼
                        print(f"æœªæ‰¾åˆ°time_stepï¼Œä½¿ç”¨é»˜è®¤å“åº”æ—¶é—´={info['response_time']}")
        
        # æ›´æ–°å½“å‰æ—¶é—´æ­¥
        self.current_time_step += 1
        
        return next_state, reward, done, info


# è®­ç»ƒMARLç³»ç»Ÿ
def train_marl(env, controller, num_episodes, max_steps, with_verbose=False, save_freq=5):
    """
    è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œç”¨äºMARLè®­ç»ƒ
    """
    print("å¼€å§‹MARLè®­ç»ƒè¿‡ç¨‹...")
    print("-------------------------------------------")
    print("è®­ç»ƒåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š")
    print(" - é˜¶æ®µ1ï¼šç¾éš¾åˆæœŸï¼Œç”Ÿæˆæ¦‚ç‡çº¦0.5ï¼Œç»´æŒ20-50ä¸ªç¾éš¾ç‚¹ï¼ˆå°‘äº20ä¸ªè‡ªåŠ¨è¡¥å……åˆ°20ä¸ªï¼Œå¤šäº50ä¸ªè‡ªåŠ¨å‡å°‘åˆ°50ä¸ªï¼‰")
    print(" - é˜¶æ®µ2ï¼šç¾éš¾ä¸­æœŸï¼Œç”Ÿæˆæ¦‚ç‡çº¦0.3ï¼Œç»´æŒ5-20ä¸ªç¾éš¾ç‚¹ï¼ˆå°‘äº5ä¸ªè‡ªåŠ¨è¡¥å……åˆ°5ä¸ªï¼Œå¤šäº20ä¸ªè‡ªåŠ¨å‡å°‘åˆ°20ä¸ªï¼‰")
    print(" - é˜¶æ®µ3ï¼šç¾éš¾åæœŸï¼Œç”Ÿæˆæ¦‚ç‡çº¦0.1ï¼Œç»´æŒ1-5ä¸ªç¾éš¾ç‚¹ï¼ˆå°‘äº1ä¸ªè‡ªåŠ¨è¡¥å……åˆ°1ä¸ªï¼Œå¤šäº5ä¸ªè‡ªåŠ¨å‡å°‘åˆ°5ä¸ªï¼‰")
    print("-------------------------------------------")
    
    # å­˜å‚¨æ¯ä¸ªå›åˆçš„å¥–åŠ±ï¼Œç”¨äºè®¡ç®—å¹³å‡å¥–åŠ±
    all_rewards = []
    success_rates = []
    response_times = []
    losses = []  # æ·»åŠ æŸå¤±è¿½è¸ª
    
    # æ–°å¢ï¼šç”¨äºé«˜çº§å¯è§†åŒ–çš„ç¯å¢ƒå¿«ç…§åˆ—è¡¨
    env_snapshots = []
    
    # ä¿å­˜åˆå§‹æ•‘æ´äººå‘˜ä¿¡æ¯
    initial_rescuers = env.rescuers
    model_path = config.MARL_CONFIG['model_save_path']
    model_dir = os.path.dirname(model_path)
    rescuers_data_path = os.path.join(model_dir, "rescuers_data.json")
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(model_dir, exist_ok=True)
    
    # ä½¿ç”¨RescueEnvironmentæ¥ä¿å­˜æ•‘æ´äººå‘˜æ•°æ®
    from src.rl.marl_rescue import RescueEnvironment
    temp_env = RescueEnvironment(grid_size=env.GRID_SIZE, rescuers_data=initial_rescuers)
    temp_env.save_rescuers_data(rescuers_data_path)
    print(f"å·²ä¿å­˜åˆå§‹æ•‘æ´äººå‘˜æ•°æ®åˆ°: {rescuers_data_path}")

    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    models_dir = os.path.join(model_dir, "checkpoints")
    os.makedirs(models_dir, exist_ok=True)
    print(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {models_dir}")
    
    # å¼€å§‹è®­ç»ƒ
    for episode in range(num_episodes):
        total_reward = 0
        success_count = 0
        total_response_time = 0
        disaster_count = 0  # æ”¹ä¸ºç»Ÿè®¡æœ¬episodeä¸­æ€»å…±å‡ºç°çš„ä¸åŒç¾éš¾æ•°é‡
        total_rescue_attempts = 0  # æ–°å¢ï¼šç»Ÿè®¡æ€»æ•‘æ´å°è¯•æ¬¡æ•°
        episode_loss = 0  # è®°å½•æœ¬å›åˆçš„å¹³å‡æŸå¤±
        seen_disasters = set()  # æ–°å¢ï¼šè®°å½•æœ¬episodeä¸­å‡ºç°è¿‡çš„æ‰€æœ‰ç¾éš¾ID
        
        # é‡ç½®ç¯å¢ƒ - åˆ›å»ºæ–°çš„ç¯å¢ƒå®ä¾‹è€Œä¸æ˜¯è°ƒç”¨resetæ–¹æ³•
        if episode > 0:  # åªæœ‰åœ¨ç¬¬äºŒè½®å¼€å§‹æ—¶æ‰éœ€è¦é‡ç½®ï¼Œå› ä¸ºç¬¬ä¸€è½®å·²ç»æœ‰åˆå§‹ç¯å¢ƒ
            # ä½¿ç”¨ç›¸åŒçš„æ•‘æ´äººå‘˜æ•°æ®é‡ç½®ç¯å¢ƒï¼Œç¡®ä¿æ•‘æ´äººå‘˜å‚æ•°ä¿æŒå›ºå®šï¼Œå¹¶å¯ç”¨è®­ç»ƒæ¨¡å¼
            env = Environment(verbose=False, rescuers_data=initial_rescuers, training_mode=True)  # ä½¿ç”¨æ— è¾“å‡ºç‰ˆæœ¬ï¼Œå¯ç”¨è®­ç»ƒæ¨¡å¼
            # æ›´æ–°ç¯å¢ƒç¼“å­˜
            if hasattr(controller, "_env_cache"):
                controller._env_cache = [env]
        
        # è°ƒæ•´æ¢ç´¢ç‡
        epsilon_progress = min(1.0, episode / (0.8 * num_episodes))
        epsilon_start = controller.epsilon_start  # ä½¿ç”¨æ§åˆ¶å™¨çš„epsilon_start
        epsilon_end = controller.epsilon_end  # ä½¿ç”¨æ§åˆ¶å™¨çš„epsilon_end
        epsilon = max(epsilon_end, epsilon_start - epsilon_progress * (epsilon_start - epsilon_end))
        controller.epsilon = epsilon
        
        # æ¯æ­¥ä¿å­˜ç¯å¢ƒå¿«ç…§ï¼Œä½†ä¸ç«‹å³å¯è§†åŒ–
        for step in range(max_steps):
            if with_verbose:
                print(f"  æ­¥éª¤ {step+1}/{max_steps}...")
            
            # æ¯ä¸ªæ­¥éª¤éƒ½è°ƒç”¨adjust_disaster_settingsä»¥å®ç°ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥
            adjust_disaster_settings(env, step, max_steps, verbose=(step % 50 == 0))
            
            # æ¯10æ­¥è¾“å‡ºå½“å‰ç¾éš¾ç‚¹æ•°é‡
            if step % 10 == 0:
                # æ ¹æ®ç¯å¢ƒæ¥å£è·å–æ´»è·ƒç¾éš¾ç‚¹æ•°é‡ï¼ˆåªè®¡ç®—éœ€è¦æ•‘æ´çš„ç‚¹ï¼‰
                if hasattr(env, "disasters"):
                    current_active_disasters = sum(1 for d in env.disasters.values() if d.get("rescue_needed", 0) > 0)
                    total_disasters = len(env.disasters)
                    high_level = sum(1 for d in env.disasters.values() if d["level"] >= 9 and d.get("rescue_needed", 0) > 0)
                    medium_level = sum(1 for d in env.disasters.values() if 7 <= d["level"] < 9 and d.get("rescue_needed", 0) > 0)
                    low_level = sum(1 for d in env.disasters.values() if d["level"] < 7 and d.get("rescue_needed", 0) > 0)
                else:
                    # å¦‚æœç¯å¢ƒæ¥å£ä¸å…¼å®¹ï¼Œåˆ™è®¾ç½®ä¸º0
                    current_active_disasters = 0
                    total_disasters = 0
                    high_level = medium_level = low_level = 0
                
                # æ˜¾ç¤ºæ´»è·ƒç¾éš¾ç‚¹æ•°é‡å’Œæ€»æ•°
                print(f"{Colors.CYAN}[æ­¥éª¤ {step}/{max_steps}] æ´»è·ƒç¾éš¾ç‚¹: {current_active_disasters} (æ€»è®¡: {total_disasters}) " +
                      f"(é«˜é£é™©: {Colors.RED}{high_level}{Colors.CYAN}, " +
                      f"ä¸­é£é™©: {Colors.YELLOW}{medium_level}{Colors.CYAN}, " +
                      f"ä½é£é™©: {Colors.GREEN}{low_level}{Colors.ENDC})")
                
                # æ¯20æ­¥æ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                if step % 20 == 0:
                    # æ­£ç¡®è®¡ç®—å·²è§£å†³å’Œå¤±è´¥çš„ç¾éš¾ç‚¹
                    resolved_disasters = sum(1 for d in env.disasters.values() 
                                           if d.get("rescue_needed", 0) == 0 and d.get("rescue_success", False) == True)
                    failed_disasters = sum(1 for d in env.disasters.values() 
                                         if d.get("rescue_needed", 0) == 0 and d.get("rescue_success", False) == False)
                    
                    print(f"    ğŸ” è°ƒè¯•ï¼šæ€»è®¡={total_disasters}, æ´»è·ƒ={current_active_disasters}, å·²è§£å†³={resolved_disasters}, å¤±è´¥={failed_disasters}")
                    print(f"    ğŸ” è°ƒè¯•ï¼šæ´»è·ƒ+å·²è§£å†³+å¤±è´¥={current_active_disasters + resolved_disasters + failed_disasters}, åº”è¯¥ç­‰äºæ€»è®¡{total_disasters}")
                    
                    # æ£€æŸ¥å¼‚å¸¸æƒ…å†µ
                    if current_active_disasters + resolved_disasters + failed_disasters != total_disasters:
                        print(f"    âš ï¸ ç»Ÿè®¡å¼‚å¸¸ï¼šæ•°é‡ä¸åŒ¹é…ï¼")
                        # æŸ¥æ‰¾å¼‚å¸¸çš„ç¾éš¾ç‚¹
                        anomaly_count = 0
                        for pos, d in env.disasters.items():
                            if d.get("rescue_needed", 0) == 0 and d.get("frozen_rescue", False) == False:
                                anomaly_count += 1
                        if anomaly_count > 0:
                            print(f"    ğŸ” å‘ç°{anomaly_count}ä¸ªå¼‚å¸¸ç¾éš¾ç‚¹ï¼šrescue_needed=0ä½†frozen_rescue=False")
            
            # æ›´æ–°ç¾éš¾çŠ¶æ€ï¼ˆä½¿ç”¨æ— è°ƒè¯•è¾“å‡ºæ¨¡å¼ï¼‰
            with io.StringIO() as buf, contextlib.redirect_stdout(buf):
                if hasattr(env, 'update_disasters_silent'):
                    env.update_disasters_silent(current_time_step=env.current_time_step)
                else:
                    env.update_disasters(current_time_step=env.current_time_step)
            
            # å¢åŠ ç¯å¢ƒçš„å½“å‰æ—¶é—´æ­¥ï¼Œç¡®ä¿æ—¶é—´æ­£ç¡®æ¨è¿›
            env.current_time_step = env.current_time_step + 1 if hasattr(env, 'current_time_step') else step
            
            # ç»Ÿè®¡æœ¬episodeä¸­å‡ºç°è¿‡çš„æ‰€æœ‰ä¸åŒç¾éš¾
            if hasattr(env, "disasters"):
                current_disaster_ids = set(env.disasters.keys())
                seen_disasters.update(current_disaster_ids)
                # ä½¿ç”¨å½“å‰æ´»è·ƒç¾éš¾ç‚¹æ•°é‡ï¼Œè€Œä¸æ˜¯ç´¯ç§¯è§è¿‡çš„ä¸åŒç¾éš¾æ€»æ•°
                total_disasters = len(env.disasters)  # å½“å‰æ€»ç¾éš¾ç‚¹æ•°ï¼ˆåŒ…æ‹¬æ‰€æœ‰çŠ¶æ€ï¼‰
                active_disasters = sum(1 for d in env.disasters.values() if d.get("rescue_needed", 0) > 0)  # å½“å‰æ´»è·ƒç¾éš¾ç‚¹æ•°
            
            # åœ¨æ¯ä¸ªstepå¼€å§‹æ—¶è®°å½•æ­¥éª¤å¼€å§‹æ—¶çš„çŠ¶æ€
            step_start_success_count = success_count
            step_start_rescue_attempts = total_rescue_attempts
            
            # éå†æ¯ä¸ªæ•‘æ´è€…æ™ºèƒ½ä½“
            # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰æ•‘æ´äººå‘˜çš„çŠ¶æ€å’ŒåŠ¨ä½œï¼Œä½†ä¸ç«‹å³æ‰§è¡Œ
            actions_and_states = []
            old_states = []
            old_disasters = {pos: disaster.copy() for pos, disaster in env.disasters.items()}
            
            for rescuer_idx in range(config.NUM_RESCUERS):
                if with_verbose:
                    print(f"    å¤„ç†æ•‘æ´è€… {rescuer_idx+1}/{config.NUM_RESCUERS}...")
                    
                # è·å–å½“å‰çŠ¶æ€
                state = env.get_state_for_rescuer(rescuer_idx)
                old_states.append(env.rescuers[rescuer_idx].copy() if rescuer_idx < len(env.rescuers) else {})
                
                # é€‰æ‹©åŠ¨ä½œï¼Œä¼ é€’ç¾æƒ…ä¿¡æ¯
                action = controller.select_action(state, rescuer_idx, env.disasters)
                
                # è®°å½•æ˜¯å¦æ˜¯æ•‘æ´å°è¯•ï¼ˆåŸºäºactionå’Œtargetï¼‰
                is_rescue_attempt = False
                if action > 0:  # é0åŠ¨ä½œè¡¨ç¤ºå‰å¾€æŸä¸ªä½ç½®
                    action_idx = action - 1
                    x = action_idx // env.GRID_SIZE
                    y = action_idx % env.GRID_SIZE
                    target_pos = (x, y)
                    # å¦‚æœç›®æ ‡ä½ç½®æœ‰ç¾æƒ…ï¼Œåˆ™è®¤ä¸ºæ˜¯æ•‘æ´å°è¯•
                    if target_pos in env.disasters:
                        is_rescue_attempt = True
                        total_rescue_attempts += 1
                
                # å­˜å‚¨çŠ¶æ€å’ŒåŠ¨ä½œï¼Œä½†ä¸ç«‹å³æ‰§è¡Œ
                actions_and_states.append({
                    'rescuer_idx': rescuer_idx,
                    'state': state,
                    'action': action,
                    'is_rescue_attempt': is_rescue_attempt
                })
            
            # ç¬¬äºŒé˜¶æ®µï¼šå°†æ‰€æœ‰åŠ¨ä½œè½¬æ¢ä¸ºç›®æ ‡åˆ†é…ï¼ˆç±»ä¼¼éƒ¨ç½²æ—¶çš„ä»»åŠ¡åˆ†é…ï¼‰
            for action_info in actions_and_states:
                rescuer_idx = action_info['rescuer_idx']
                action = action_info['action']
                
                if action > 0:  # é0åŠ¨ä½œè¡¨ç¤ºå‰å¾€æŸä¸ªä½ç½®
                    action_idx = action - 1
                    x = action_idx // env.GRID_SIZE
                    y = action_idx % env.GRID_SIZE
                    target_pos = (x, y)
                    
                    # å¦‚æœç›®æ ‡ä½ç½®æœ‰ç¾æƒ…ï¼Œåˆ™åˆ†é…ä»»åŠ¡
                    if target_pos in env.disasters:
                        env.rescuers[rescuer_idx]["target"] = target_pos
                    else:
                        # å¦‚æœç›®æ ‡ä½ç½®æ²¡æœ‰ç¾æƒ…ï¼Œåˆ™æ¸…é™¤å½“å‰ä»»åŠ¡
                        if "target" in env.rescuers[rescuer_idx]:
                            del env.rescuers[rescuer_idx]["target"]
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šç»Ÿä¸€æ‰§è¡Œæ•‘æ´ï¼ˆä¸éƒ¨ç½²é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
            with io.StringIO() as buf, contextlib.redirect_stdout(buf):
                from src.core.rescue_execution import execute_rescue_silent
                execute_rescue_silent(env.rescuers, env.disasters, env.GRID_SIZE, current_time_step=env.current_time_step)
            
            # ç¬¬å››é˜¶æ®µï¼šè®¡ç®—æ¯ä¸ªæ•‘æ´äººå‘˜çš„å¥–åŠ±å’Œä¸‹ä¸€çŠ¶æ€
            for action_info in actions_and_states:
                rescuer_idx = action_info['rescuer_idx']
                state = action_info['state']
                action = action_info['action']
                old_state = old_states[rescuer_idx]
                
                # è·å–æ–°çŠ¶æ€
                next_state = env.get_state_for_rescuer(rescuer_idx)
                
                # è®¡ç®—å¥–åŠ±
                try:
                    from src.rl.rl_util import calculate_reward
                    reward, reward_info = calculate_reward(env, rescuer_idx, old_state, old_disasters)
                except Exception as e:
                    print(f"[é”™è¯¯] è®¡ç®—å¥–åŠ±æ—¶å‡ºé”™: {e}")
                    reward = -0.01
                    reward_info = {"time_penalty": -0.01}
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                done = env.is_episode_done()
                
                # ç¡®å®šæˆåŠŸæ ‡å¿—å’Œå“åº”æ—¶é—´
                success = False
                response_time = 0
                
                # å¦‚æœæœ‰ç›®æ ‡ä¸”ç›®æ ‡æ˜¯ç¾æƒ…ç‚¹ä¸”ç¾æƒ…å·²è§£å†³æˆ–å‡è½»ï¼Œåˆ™è®¡ç®—å“åº”æ—¶é—´
                if "target" in env.rescuers[rescuer_idx] and env.rescuers[rescuer_idx]["target"] is not None:
                    target = env.rescuers[rescuer_idx]["target"]
                    if target in old_disasters and target in env.disasters:
                        old_disaster = old_disasters[target]
                        current_disaster = env.disasters[target]
                        
                        # å¦‚æœæ•‘æ´å–å¾—äº†è¿›å±•
                        if old_disaster["rescue_needed"] > current_disaster["rescue_needed"]:
                            success = True
                            success_count += 1
                            
                            # è®¡ç®—å“åº”æ—¶é—´ - ä½¿ç”¨æ—¶é—´æ­¥è€Œä¸æ˜¯å®é™…æ—¶é—´
                            if "time_step" in current_disaster:
                                response_time = env.current_time_step - current_disaster["time_step"]
                                # ç¡®ä¿å“åº”æ—¶é—´ä¸ºæ­£æ•°
                                if response_time <= 0:
                                    response_time = 1  # ç¡®ä¿è‡³å°‘ä¸º1
                                total_response_time += response_time
                
                # å¤„ç†å¥–åŠ±ä¿¡æ¯
                total_reward += reward
                
                # å­˜å‚¨è½¬æ¢åˆ°ç»éªŒå›æ”¾ç¼“å†²åŒº
                controller.store_transition(state, action, reward, next_state, done, rescuer_idx)
            
            # æ›´æ–°ç¥ç»ç½‘ç»œ
            loss = controller.update_agents()
            episode_loss += loss  # ç´¯è®¡æŸå¤±
            
            # ä¿å­˜ç¯å¢ƒå¿«ç…§ - æ¯ä¸€æ­¥éƒ½ä¿å­˜
            env_copy = copy.deepcopy(env)
            
            # ä¿®å¤ï¼šä½¿ç”¨main.pyä¸­çš„æˆåŠŸç‡è®¡ç®—ç®—æ³•
            # ä½¿ç”¨calculate_rescue_success_rateå‡½æ•°ï¼ŒåŸºäºæœ€è¿‘å·²å®Œæˆçš„ç¾æƒ…ç‚¹è®¡ç®—æˆåŠŸç‡
            current_success_rate = calculate_rescue_success_rate(
                env.disasters, 
                window=config.STATS_WINDOW_SIZE, 
                current_time_step=env.current_time_step
            )
            
            # åˆ›å»ºå¿«ç…§å­—å…¸
            snapshot = {
                "env": env_copy,
                "time_step": episode * max_steps + step,  # è®¡ç®—æ€»æ—¶é—´æ­¥
                "success_rate": current_success_rate,
                "episode": episode + 1,
                "step": step + 1,
                # æ·»åŠ è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
                "disaster_count": total_disasters,  # ä¿®æ”¹ä¸ºæ€»ç¾éš¾æ•°
                "active_disaster_count": active_disasters,  # æ–°å¢æ´»è·ƒç¾éš¾æ•°
                "seen_disasters_count": len(seen_disasters),  # æ–°å¢ï¼šç´¯ç§¯è§è¿‡çš„ä¸åŒç¾éš¾æ€»æ•°
                "success_count": success_count,
                "total_rescue_attempts": total_rescue_attempts,
                "total_reward": total_reward,
                "avg_reward": total_reward / config.NUM_RESCUERS if config.NUM_RESCUERS > 0 else 0,
                "avg_response_time": total_response_time / max(1, success_count)
            }
            
            # æ·»åŠ åˆ°å¿«ç…§åˆ—è¡¨
            env_snapshots.append(snapshot)
        
        # è®¡ç®—å¹³å‡æŸå¤± - ç¡®ä¿ä¸ä¸ºé›¶
        steps_completed = min(step + 1, max_steps)  # ä½¿ç”¨å®é™…å®Œæˆçš„æ­¥æ•°
        avg_loss = episode_loss / steps_completed if steps_completed > 0 else 0
        
        # ä¸å†å¯¹æŸå¤±è¿›è¡Œè£å‰ª
        losses.append(avg_loss)
        
        # è®¡ç®—å¹³å‡å¥–åŠ±å’Œæœ€ç»ˆæˆåŠŸç‡
        avg_reward = total_reward / config.NUM_RESCUERS if config.NUM_RESCUERS > 0 else 0
        
        # ä½¿ç”¨ä¸main.pyä¸€è‡´çš„æˆåŠŸç‡è®¡ç®—ç®—æ³•
        # åŸºäºæœ€è¿‘å·²å®Œæˆçš„ç¾æƒ…ç‚¹è®¡ç®—æœ€ç»ˆæˆåŠŸç‡
        final_success_rate = calculate_rescue_success_rate(
            env.disasters, 
            window=config.STATS_WINDOW_SIZE, 
            current_time_step=env.current_time_step
        )
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´ - ä¿®å¤è®¡ç®—é€»è¾‘
        avg_response_time = total_response_time / max(1, success_count)  # ç¡®ä¿åˆ†æ¯éé›¶
        
        all_rewards.append(avg_reward)
        success_rates.append(final_success_rate)  # ä½¿ç”¨æœ€ç»ˆæˆåŠŸç‡
        response_times.append(avg_response_time)
        
        # æ¯éš”ä¸€å®šå›åˆä¿å­˜æ¨¡å‹
        if (episode + 1) % save_freq == 0:
            # ä¿å­˜å¸¦æœ‰è½®æ¬¡ç¼–å·çš„æ¨¡å‹
            checkpoint_path = os.path.join(models_dir, f"model_episode_{episode+1}.pt")
            controller.save_models(checkpoint_path)
            print(f"[è¿›åº¦ {episode+1}/{num_episodes}] å·²ä¿å­˜æ¨¡å‹åˆ° {checkpoint_path} (å®Œæˆ: {(episode+1)/num_episodes*100:.1f}%)")
            
            # åŒæ—¶ä¿å­˜åˆ°é»˜è®¤è·¯å¾„ï¼Œç¡®ä¿å§‹ç»ˆæœ‰æœ€æ–°çš„æ¨¡å‹å¯ç”¨
            controller.save_models()
            print(f"å·²å°†æœ€æ–°æ¨¡å‹ä¿å­˜åˆ°é»˜è®¤è·¯å¾„: {config.MARL_CONFIG['model_save_path']}")
        
        # æ¯è½®è¾“å‡ºä¸€æ¬¡ç»¼åˆä¿¡æ¯ - ä½¿ç”¨æœ€ç»ˆæˆåŠŸç‡
        print(f"[è½®æ¬¡ {episode+1}/{num_episodes}] æ¢ç´¢ç‡: {epsilon:.4f}, å¹³å‡å¥–åŠ±: {avg_reward:.2f}, " +
              f"æˆåŠŸç‡: {final_success_rate:.2f}, å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’, å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # æ¯è½®éƒ½è¾“å‡ºæœ¬è½®è®­ç»ƒè¯¦ç»†æ•°æ®
        print(f"\n----- æœ¬è½®è®­ç»ƒè¯¦ç»†æ•°æ® -----")
        print(f"â€¢ å‡ºç°çš„ä¸åŒç¾éš¾æ€»æ•°: {disaster_count}")
        print(f"â€¢ æ•‘æ´å°è¯•æ¬¡æ•°: {total_rescue_attempts}")
        print(f"â€¢ æˆåŠŸæ•‘æ´æ¬¡æ•°: {success_count}")
        print(f"â€¢ æœ€ç»ˆæˆåŠŸç‡ (åŸºäºå·²å®Œæˆç¾æƒ…ç‚¹): {final_success_rate:.2%}")
        print(f"â€¢ æ€»å¥–åŠ±: {total_reward:.2f}")
        print(f"â€¢ è®­ç»ƒæ­¥æ•°: {steps_completed}")
        print(f"----- æœ¬è½®è®­ç»ƒè¯¦ç»†æ•°æ® -----\n")
        
        # æ¯10ä¸ªå›åˆè¾“å‡ºä¸€æ¬¡æœ€è¿‘100å›åˆçš„ç»Ÿè®¡ä¿¡æ¯
        if (episode + 1) % 10 == 0 or episode == num_episodes - 1:
            recent_rewards = all_rewards[-100:] if len(all_rewards) >= 100 else all_rewards
            recent_success = success_rates[-100:] if len(success_rates) >= 100 else success_rates
            recent_response = response_times[-100:] if len(response_times) >= 100 else response_times
            recent_losses = losses[-100:] if len(losses) >= 100 else losses
            
            print("\n========== æœ€è¿‘è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ ==========")
            print(f"æœ€è¿‘{len(recent_rewards)}å›åˆç»Ÿè®¡:")
            print(f"â€¢ å¹³å‡å¥–åŠ±: {np.mean(recent_rewards):.2f}")
            print(f"â€¢ å¹³å‡æˆåŠŸç‡: {np.mean(recent_success):.2f}")
            print(f"â€¢ å¹³å‡å“åº”æ—¶é—´: {np.mean(recent_response):.2f}ç§’")
            print(f"â€¢ å¹³å‡æŸå¤±: {np.mean(recent_losses):.4f}")
            print(f"â€¢ å½“å‰æ¢ç´¢ç‡: {epsilon:.4f}")
            print(f"â€¢ æœ€é«˜æˆåŠŸç‡: {max(recent_success) if recent_success else 0:.2f}")
            
            # æ·»åŠ å¥–åŠ±åˆ†è§£ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(controller, "get_reward_stats"):
                reward_stats = controller.get_reward_stats(reset=False)
                print(f"\nå¥–åŠ±åˆ†è§£:")
                for reward_type, stats in reward_stats.items():
                    if stats["total"] != 0:  # åªæ˜¾ç¤ºéé›¶å¥–åŠ±
                        print(f"â€¢ {reward_type}: æ€»è®¡={stats['total']:.2f}, å¹³å‡={stats['avg']:.2f}")
            
            print("==================================\n")
        
        # æ¯è½®ç»“æŸæ—¶ä¿å­˜å…ƒæ•°æ®ï¼Œç®€åŒ–é€»è¾‘ï¼Œç§»é™¤å¯è§†åŒ–éƒ¨åˆ†
        print(f"\n[å…ƒæ•°æ®] æ­£åœ¨ä¿å­˜ç¬¬ {episode+1} è½®çš„è®­ç»ƒå…ƒæ•°æ®...")

        # åˆ›å»ºå…ƒæ•°æ®ä¿å­˜ç›®å½•ç»“æ„
        metadata_dir = os.path.join(SAVE_DIR, "metadata")
        os.makedirs(metadata_dir, exist_ok=True)

        # åˆ›å»ºè¯¥è½®çš„å…ƒæ•°æ®æ–‡ä»¶åï¼ˆç¡®ä¿ä¸ä¼šè¦†ç›–ï¼‰
        episode_meta_file = os.path.join(metadata_dir, f"episode_{episode+1:04d}.json")

        # æå–å½“å‰è½®çš„å¿«ç…§å…ƒæ•°æ®
        episode_meta = []
        for snapshot in env_snapshots:
            if snapshot["episode"] == episode + 1:
                # ä¿å­˜å…ƒæ•°æ®
                meta = {
                    "time_step": snapshot["time_step"],
                    "success_rate": snapshot["success_rate"],
                    "episode": snapshot["episode"],
                    "step": snapshot["step"],
                    # ä½¿ç”¨å¿«ç…§ä¸­ä¿å­˜çš„æ­£ç¡®æ•°æ®ï¼Œè€Œä¸æ˜¯episodeçº§åˆ«çš„ç´¯ç§¯å€¼
                    "disaster_count": snapshot["disaster_count"],  # æœ¬episodeä¸­å‡ºç°è¿‡çš„ä¸åŒç¾éš¾æ€»æ•°
                    "active_disaster_count": snapshot["active_disaster_count"],  # æ–°å¢æ´»è·ƒç¾éš¾æ•°
                    "seen_disasters_count": snapshot["seen_disasters_count"],  # æ–°å¢ï¼šç´¯ç§¯è§è¿‡çš„ä¸åŒç¾éš¾æ€»æ•°
                    "success_count": snapshot["success_count"],    # æˆªè‡³è¯¥æ­¥éª¤çš„æˆåŠŸæ•‘æ´æ¬¡æ•°
                    "total_rescue_attempts": snapshot["total_rescue_attempts"],  # æˆªè‡³è¯¥æ­¥éª¤çš„æ€»æ•‘æ´å°è¯•æ¬¡æ•°
                    "total_reward": snapshot["total_reward"],     # æˆªè‡³è¯¥æ­¥éª¤çš„æ€»å¥–åŠ±
                    "avg_reward": snapshot["avg_reward"],         # æˆªè‡³è¯¥æ­¥éª¤çš„å¹³å‡å¥–åŠ±
                    "avg_response_time": snapshot["avg_response_time"]  # æˆªè‡³è¯¥æ­¥éª¤çš„å¹³å‡å“åº”æ—¶é—´
                }
                episode_meta.append(meta)

        # ä¿å­˜å…ƒæ•°æ®ï¼ˆæŒ‰æ­¥éª¤æ’åºï¼‰
        episode_meta.sort(key=lambda x: x["step"])
        with open(episode_meta_file, 'w') as f:
            json.dump({
                "metadata": episode_meta,
                "env_config": {
                    "grid_size": env.GRID_SIZE,
                    "num_rescuers": len(env.rescuers),
                    "max_steps": max_steps
                },
                "metrics": {
                    "rewards": all_rewards[:episode+1],
                    "success_rates": success_rates[:episode+1],
                    "response_times": response_times[:episode+1],
                    "losses": losses[:episode+1] if losses else []
                },
                "episode_summary": {
                    "total_disasters": total_disasters,
                    "total_rescue_attempts": total_rescue_attempts,
                    "successful_rescues": success_count,
                    "success_rate": final_success_rate,
                    "avg_reward": avg_reward,
                    "avg_response_time": avg_response_time
                }
            }, f)

        print(f"[å…ƒæ•°æ®] ç¬¬ {episode+1} è½®è®­ç»ƒå…ƒæ•°æ®å·²ä¿å­˜åˆ°: {episode_meta_file}")

        # ä¿å­˜è®­ç»ƒç¯å¢ƒå¿«ç…§ï¼Œç”¨äºåç»­å¯èƒ½çš„å¯è§†åŒ–
        snapshot_dir = os.path.join(SAVE_DIR, "snapshots", f"episode_{episode+1:04d}")
        os.makedirs(snapshot_dir, exist_ok=True)

        # ä¿å­˜æ¯ä¸€æ­¥çš„ç¯å¢ƒå¿«ç…§ï¼Œè€Œä¸åªæ˜¯æœ€åä¸€æ­¥
        if len(env_snapshots) > 0:
            print(f"[å¿«ç…§] å¼€å§‹ä¿å­˜ç¬¬ {episode+1} è½®çš„æ‰€æœ‰æ­¥éª¤å¿«ç…§...")
            
            # ç­›é€‰å‡ºå½“å‰è½®çš„æ‰€æœ‰å¿«ç…§
            current_episode_snapshots = [s for s in env_snapshots if s["episode"] == episode + 1]
            
            # ä¸ºæ¯ä¸ªæ­¥éª¤ä¿å­˜ç‹¬ç«‹çš„å¿«ç…§æ–‡ä»¶
            for i, snapshot in enumerate(current_episode_snapshots):
                step_num = snapshot.get("step", i + 1)
                snapshot_file = os.path.join(snapshot_dir, f"step_{step_num:04d}.pkl")
                
                # ä¿å­˜å¿«ç…§
                with open(snapshot_file, 'wb') as f:
                    pickle.dump(snapshot, f)
                
                # æ¯10æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                if step_num % 10 == 0 or step_num == len(current_episode_snapshots):
                    print(f"[å¿«ç…§] å·²ä¿å­˜æ­¥éª¤ {step_num}/{len(current_episode_snapshots)} åˆ°: {snapshot_file}")
            
            # åŒæ—¶ä¿ç•™æœ€åä¸€æ­¥çš„å¿«ç…§ä½œä¸ºfinal_state.pklï¼Œä¿æŒå‘åå…¼å®¹
            if current_episode_snapshots:
                final_snapshot = current_episode_snapshots[-1]
                final_snapshot_file = os.path.join(snapshot_dir, "final_state.pkl")
                with open(final_snapshot_file, 'wb') as f:
                    pickle.dump(final_snapshot, f)
                print(f"[å¿«ç…§] å·²ä¿å­˜æœ€ç»ˆçŠ¶æ€å¿«ç…§åˆ°: {final_snapshot_file}")
            
            print(f"[å¿«ç…§] ç¬¬ {episode+1} è½®å…±ä¿å­˜äº† {len(current_episode_snapshots)} ä¸ªæ­¥éª¤å¿«ç…§")

        # åªä¿ç•™æœ€æ–°ä¸€è½®çš„ç¯å¢ƒå¿«ç…§ï¼Œé‡Šæ”¾å†…å­˜
        if episode > 0:  # ç¬¬ä¸€è½®ä¹‹åæ‰æ¸…ç†
            # ç­›é€‰å‡ºå½“å‰è½®çš„å¿«ç…§ï¼Œé‡Šæ”¾ä¹‹å‰è½®çš„å¿«ç…§
            env_snapshots = [s for s in env_snapshots if s["episode"] == episode + 1]
            print(f"[å†…å­˜] å·²æ¸…ç†æ—§è½®æ¬¡ç¯å¢ƒå¿«ç…§ï¼Œå½“å‰ä¿ç•™ {len(env_snapshots)} ä¸ªå¿«ç…§")
    
    return all_rewards, success_rates, response_times




# è¯„ä¼°MARLç³»ç»Ÿ
def evaluate_marl(env, episodes=5, max_steps=config.SIMULATION_TIME):
    """è¯„ä¼°MARLç³»ç»Ÿæ€§èƒ½"""
    # åˆ›å»ºMARLæ§åˆ¶å™¨
    marl = MARLController(
        env_or_grid_size=env.GRID_SIZE,
        num_rescuers=len(env.rescuers),
        hidden_dim=config.MARL_CONFIG["hidden_dim"],
        lr=config.MARL_CONFIG["learning_rate"],
        gamma=config.MARL_CONFIG["gamma"]
    )
    
    # åŠ è½½æ¨¡å‹
    if not marl.load_models():
        print("æ— æ³•åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨éšæœºç­–ç•¥è¿›è¡Œè¯„ä¼°")
    
    # åˆ›å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒåŒ…è£…
    rl_env = RescueEnvironment(env)
    
    # è®°å½•è¯„ä¼°æ•°æ®
    all_success_rates = []
    all_avg_response_times = []
    
    for episode in range(episodes):
        # é‡ç½®ç¯å¢ƒ
        env = rl_env.reset()
        
        for step in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œï¼ˆè¯„ä¼°æ¨¡å¼ï¼Œä¸ä½¿ç”¨æ¢ç´¢ï¼‰
            actions = marl.select_actions(env.rescuers, env.disasters, training=False)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            env, _, _, done, _ = rl_env.step(actions, step)
            
            if done:
                break
        
        # è®¡ç®—æœ¬è½®è¯„ä¼°çš„ç»Ÿè®¡æ•°æ®
        from src.utils.stats import calculate_rescue_success_rate, calculate_average_response_time
        
        success_rate = calculate_rescue_success_rate(env.disasters, window=config.STATS_WINDOW_SIZE)
        avg_response_time = calculate_average_response_time(env.disasters)
        
        all_success_rates.append(success_rate)
        all_avg_response_times.append(avg_response_time)
        
        print(f"è¯„ä¼°è½®æ¬¡ {episode+1}/{episodes} | "
              f"æ•‘æ´æˆåŠŸç‡: {success_rate:.2f} | "
              f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}")
    
    # è®¡ç®—å¹³å‡æ€§èƒ½
    avg_success_rate = sum(all_success_rates) / len(all_success_rates)
    avg_response_time = sum(all_avg_response_times) / len(all_avg_response_times)
    
    print(f"\nè¯„ä¼°ç»“æœ:")
    print(f"å¹³å‡æ•‘æ´æˆåŠŸç‡: {avg_success_rate:.2f}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}")
    
    return avg_success_rate, avg_response_time 